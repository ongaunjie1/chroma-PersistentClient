{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark is a complex piece of software and most guides out there are over-complicating the\n",
      "installation proces. We’ll take a much simpler approach by installing the bare minimum to start,\n",
      "and building from there. Our goals are as follow:\n",
      "Install Java (Spark is written in Scala, which runs on the Java Virtual Machine, or JVM).\n",
      "Install Spark\n",
      "Install Python 3 and IPython\n",
      "Launch a PySpark shell using IPython\n",
      "(Optional): Install Jupyter and use it with PySpark.Appendix B: Installing PySpark locally\n",
      "209\n",
      "©Manning Publications Co.  To comment go to liveBook \n",
      "https://livebook.manning.com/#!/book/pyspark-in-action/discussionWhen working on Windows, you either have the option to install Spark directly on Windows, or\n",
      "to use WSL (Windows Subsystem for Linux). If you want to use WSL, follow the instructions at \n",
      " and then follow the instructions for GNU/Linux. If you want to install on plain aka.ms/wslinstall\n",
      "Windows, follow the rest of this section.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# Initialize Chroma DB client\n",
    "client = chromadb.PersistentClient(path=\"./db\")\n",
    "collection = client.get_collection(name=\"my_collection\")\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "# Get user input\n",
    "query = input(\"Enter your query: \")\n",
    "\n",
    "# Convert query to vector representation\n",
    "query_vector = embeddings.embed_query(query)\n",
    "\n",
    "# Query Chroma DB with the vector representation\n",
    "results = collection.query(query_embeddings=query_vector, n_results=1 , include=[\"documents\"])\n",
    "\n",
    "# Print results\n",
    "for result in results[\"documents\"]:\n",
    "    for i in result:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passing it through gemini-pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma DB client\n",
    "client = chromadb.PersistentClient(path=\"./db\")\n",
    "collection = client.get_collection(name=\"my_collection\")\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")\n",
    "\n",
    "# Get user input\n",
    "query = \"Why use pyspark?\"\n",
    "\n",
    "# Convert query to vector representation\n",
    "query_vector = embeddings.embed_query(query)\n",
    "\n",
    "# Query Chroma DB with the vector representation\n",
    "results = collection.query(query_embeddings=query_vector, n_results=3 , include=[\"documents\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf_10',\n",
       "   'Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf_11',\n",
       "   'Jonathan Rioux - Data Analysis with Python and PySpark-Manning (2022).pdf_30']],\n",
       " 'distances': None,\n",
       " 'metadatas': None,\n",
       " 'embeddings': None,\n",
       " 'documents': [['providing a coherent interface between language while preserving the idiosyncrasies of the\\nlanguage where appropriate. Your PySpark program will therefore by quite easy to read by a\\nScala/Spark programmer, but also to a fellow Python programmer who hasn’t jumped into the\\ndeep end (yet).\\nPython is a dynamic, general purpose language, available on many platforms and for a variety of\\ntasks. Its versatility and expressiveness makes it an especially good fit for PySpark. The\\nlanguage is one of the most popular for a variety of domains, and currently is a major force in\\ndata analysis and science. The syntax is easy to learn and read, and the amount of library\\navailable means that you’ll often find one (or more!) who’s just the right fit for your problem.\\nThere are no shortage of libraries and framework to work with data. Why should one spend their\\ntime learning PySpark specifically?\\nPySpark packs a lot of advantages for modern data workloads. It sits at the intersection of fast,',\n",
       "   'PySpark packs a lot of advantages for modern data workloads. It sits at the intersection of fast,\\nexpressive and versatile. Let’s explore those three themes one by one.\\nIf you search \"Big Data\" in a search engine, there is a very good chance that Hadoop will come\\nwithin the first few results. There is a very good reason to this: Hadoop popularized the famous \\n framework that Google pioneered in 2004 and is now a staple in Data Lakes and Big MapReduce\\nData Warehouses everywhere.\\nSpark was created a few years later, sitting on Hadoop’s incredible legacy. With an aggressive\\nquery optimizer, a judicious usage of RAM and some other improvements we’ll touch on in the\\nnext chapters, Spark can run up to 100x faster than plain Hadoop. Because of the integration\\nbetween the two frameworks, you can easily switch your Hadoop workflow to Spark and gain\\nthe performance boost without changing your hardware.1.1.2 PySpark = Spark + Python\\n1.1.3 Why PySpark?\\nPYSPARK IS FAST\\n3',\n",
       "   'Finally, I recommend that you have an analog way of drafting your code and schema. I am a\\ncompulsive note-taker and doodler, and even if my drawing are very basic and crude, I find that\\nworking through a new piece of software via drawings helps in clarifying my thoughts. This\\nmeans less code re-writing, and a happier programmer! Nothing spiffy, some scrap paper and a\\npencil will do wonders.\\nPySpark is the Python API for Spark, a distributed framework for large-scale data\\nanalysis. It provides the expressiveness and dynamism of the Python programming\\nlanguage to Spark.\\nPySpark provides a full-stack analytics workbench. It has an API for data manipulation,\\ngraph analysis, streaming data as well as machine learning.\\nSpark is fast: it owes its speed to a judicious usage of the RAM available and an\\naggressive and lazy query optimizer.\\nSpark provides bindings for Python, Scala, Java, and R. You can also use SQL for data\\nmanipulation.']],\n",
       " 'uris': None,\n",
       " 'data': None}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(results[\"documents\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark is a powerful tool for data analysis and science. It is fast, expressive, and versatile.\n",
      "\n",
      "**PySpark is fast:**\n",
      "- Spark is built on top of Hadoop, which is a distributed computing platform that can process large amounts of data in parallel.\n",
      "- Spark uses an aggressive query optimizer that can generate efficient execution plans for complex queries.\n",
      "- Spark also uses a lazy evaluation engine, which means that it only evaluates data when it is needed. This can save a lot of time and resources.\n",
      "\n",
      "**PySpark is expressive:**\n",
      "- PySpark provides a comprehensive set of APIs for data manipulation, graph analysis, streaming data, and machine learning.\n",
      "- This makes it easy to develop complex data analysis pipelines.\n",
      "\n",
      "**PySpark is versatile:**\n",
      "- PySpark can be used to process data in a variety of formats, including CSV, JSON, Parquet, and Avro.\n",
      "- It can also be used to connect to a variety of data sources, such as HDFS, Hive, and Cassandra.\n",
      "\n",
      "PySpark is a great choice for data analysis and science. It is fast, expressive, and versatile. It is also easy to learn and use, making it a good option for both experienced and novice programmers.\n",
      "\n",
      "**Why should you learn PySpark?**\n",
      "\n",
      "There are a number of reasons why you should learn PySpark, including:\n",
      "\n",
      "- **PySpark is a powerful tool for data analysis and science.** It can be used to process large amounts of data quickly and efficiently.\n",
      "- **PySpark is easy to learn and use.** The PySpark API is well-documented and there are many resources available to help you get started.\n",
      "- **PySpark is a versatile tool.** It can be used to process data in a variety of formats and it can be used to connect to a variety of data sources.\n",
      "- **PySpark is a popular tool.** It is used by many companies and organizations around the world. Learning PySpark can open up new career opportunities.\n"
     ]
    }
   ],
   "source": [
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional information: Interactive chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = model.start_chat(history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    response = chat.send_message(user_input, stream=True)\n",
    "    for chunk in response:\n",
    "        print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
